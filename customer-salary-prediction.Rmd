---
title: "Customer Salary Prediction"
author: "Collin Guidry"
date: "11/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE}
library(dplyr)
library(moments)
library(ggplot2)
library(lattice)
library(caret)
set.seed(1234)
library(naivebayes)
library(pROC)
library(caTools)
library(rpart.plot)
library(rpart)
library(randomForest)


```

```{r echo=TRUE}

#import the original adult csv from canvas here
df = read.csv("data/adult-salaries.csv")

```

```{r echo=TRUE}

#import the original adult csv from canvas here

names(df) <- c("age","workclass","fnlwgt","education", "education_num","marital_status","occupation","relationship","race","sex","capital_gain","capital_loss","hours_per_week","native_country","salary")

```

```{r}
names(df)
```

```{r echo=TRUE}


# create a data set with 500,000 by sampling the data we are given
# The probability of any row being generated is based on the "fnlwgt" column as a weight

df = sample_n( df, size = 200000, weight = fnlwgt, replace=TRUE)

#export this
write.csv(df,"data/adult_sampled.csv", row.names = FALSE)
```

```{r}

#import the data that was previously sampled. 
#We did the sampling once and stored it here, no need to do it again.

df = read.csv("data/adult_sampled.csv")

```


```{r}
print( paste('There are',nrow(df),'rows and',ncol(df),'columns') )

```


```{r}
data.frame( data_type = sapply(df, class) )
```


"education_num" can be used to designate the levels when converting "education" to a factor

```{r}
data.frame( num_unique = sapply(df, n_distinct) , 
            data_type = sapply(df, class) ) %>%
    
    filter(data_type == 'integer') %>%
    arrange(num_unique)
```

For numeric variables, produce a table of statistics including missing values, min, max, median, mean, standard deviation, skewness and kurtosis.

"capital_gain" and "capital_loss" have the highest kurtosis, yet the most common value is zero.

If we remove their outliers, we are left with all zeroes.

We can replace the zeroes with NA, remove the outliers, then add the zeroes back

```{r warning=FALSE}

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

numeric_df =  select_if(df, is.numeric)

numeric_stats <- data.frame( 
            unique = sapply(numeric_df, n_distinct),
            isNA = sapply(numeric_df, function(x) sum(is.na(x))),
            data_type = sapply(numeric_df, class),
            min = sapply(numeric_df, min),
            max = sapply(numeric_df, max),
            mean = round( sapply(numeric_df, mean) ,0),
            median = sapply(numeric_df, median),
            std = round( sapply(numeric_df, sd) ,0),
            skew = sapply(numeric_df, skewness),
            kurt = sapply(numeric_df, kurtosis),
            mode = sapply(numeric_df, getmode)
            
            ) %>%
    arrange(unique)

numeric_stats
```

Outlier removal and imputation:

We do not want to remove outliers in cases where the max value is normal or where kurtosis is low.

```{r}

# Statistics generated on the outliers

data.frame(
  num_outliers = sapply(numeric_df, function(x){
         length(boxplot.stats(x)$out) }),
  
  outlier_mean = sapply(numeric_df, function(x){
         round(mean(boxplot.stats(x)$out),0) }),
  
  outlier_min = sapply(numeric_df, function(x){
         round(min(boxplot.stats(x)$out),0) }),
  
  outlier_max = round(sapply(numeric_df, max),0)
  ) %>% 
  arrange(-num_outliers)

```

Although it's abnormal, working 99 **hours_per_week** is possible.

-   Don't remove

**capital_gain** and **capital_loss** need outliers removed, but 3 stds is not going to work. We will create **capital_net** that represents both the value gained or lost for the population.

-   Remove

The highest **education_num** ber (doctorate) is normal and the lowest is also normal. They shouldn't skew out predictions.

-   Don't remove

**fnlwgt** represents the size of the population with the row's criteria. Removing outliers is possible

A max **age** of 90 is normal.

-   Don't remove

```{r}
numeric_cols = names(numeric_df)

par(mfrow = c(1, length(numeric_cols)))
for (i in numeric_cols){
  boxplot(
    df[i],
    xlab=c(i),
    coef = 1
    )
}
```

```{r}

#make duplicate column for capital_gain
df$cg = df$capital_gain
#set zero to NA
df$cg[df$cg==0]<-NA

#make duplicate column for capital_loss
df$cl = df$capital_loss
#set zero to NA
df$cl[df$cl==0]<-NA


outlier_plot_cols = c('capital_gain','cg','capital_loss','cl')

par(mfrow = c(1, length(outlier_plot_cols)))
for (i in outlier_plot_cols){
  boxplot(
    df[i],
    xlab=c(i),
    coef = 1
    )
}
```

Boxplot shows before and after having removed zeroes. 
After removing zeroes, we will remove any values above 2.5 standard deviations above the mean to eliminate very large values.

```{r}


outlier_cols = c('cg','cl')

#remove outliers
#df[outlier_cols] <- data.frame(lapply( df[outlier_cols], function(x) {
# ifelse(x %in% boxplot.stats(x)$out, NA, x) }))


df[outlier_cols] <- data.frame( lapply(df[outlier_cols], 
  function(x, na.rm = TRUE) 
    {ifelse( (x < 0) | x > (mean(x, na.rm = TRUE) + 2 *sd(x, na.rm = TRUE)), NA, x) }))


#add zeroes back into duplicate col
df$cg [df$capital_gain==0]<-0
df$cl [df$capital_loss==0]<-0

outlier_plot_cols = c('capital_gain','cg','capital_loss','cl')

par(mfrow = c(1, length(outlier_plot_cols)))
for (i in outlier_plot_cols){
  boxplot(
    df[i],
    xlab=c(i),
    coef = 1
    )
}



```

Plot shows before vs after having removed zeroes, then removed outliers, and added zeroes back.

EDA / Sanity check :

Many values are marked with "?" -- so, where are they? and set them to NA.

Remove "?"s or no?

```{r eval=FALSE, include=FALSE}

# no need to display this any more

data.frame(
  missing =  sapply(df, function(x) sum(x==' ?') )) %>%
  arrange(-missing)

#set ? to NA
#df[df==' ?']<-NA
```

Show count of unique categorical values and NA coun

```{r}
cat_stats <- data.frame( 
            unique = sapply(df, n_distinct),
            NA_count = sapply(df, function(x) sum(is.na(x))),
            data_type = sapply(df, class)
            ) %>%
    filter(data_type == 'character') %>%
    select(unique, NA_count) %>%
    arrange(unique)
    
cat_stats
```

Any strange number of unique values?

-   Martital status should be reduced to married vs not married.

-   Education can be reduced to lower, middle, high school, some college, college, grad school.

-   Any others? Workclass?

```{r}

df = df %>% mutate(
  married = case_when(
    marital_status == ' Divorced' ~ "N",
    marital_status == ' Married-AF-spouse' ~ "Y",
    marital_status == ' Married-civ-spouse' ~ "Y",
    marital_status == ' Married-spouse-absent' ~ "Y",
    marital_status == ' Never-married' ~ "N",
    marital_status == ' Separated' ~ "Y",
    marital_status == ' Widowed' ~ "N"
  )
)


df = df %>% mutate(
  education_simple = case_when(
    education_num <=4 ~ "Below High School",
    education_num >=5 & education_num <=8 ~ "Some High School",
    education_num ==9 ~ "High School",
    education_num ==10 ~ "Some College",
    education_num >=11 & education_num <=13 ~ "College",
    education_num >=14 ~ "Masters or Above"
  
  )
)
```

Impute the missing values.

Replaced NAs with the mean.

```{r}

outlier_cols = c('cg','cl')

df[outlier_cols] <- data.frame(lapply(df[outlier_cols], function(x) {
  ifelse(is.na(x), mean(x, na.rm = TRUE), x) }))


# overwrite capital_gain and capital_loss with cg and gl, then drop cg and cl
df$capital_gain = df$cg
df$capital_loss = df$cl
df = select(df, -cg, -cl)

```

EDA, continued 

```{r}

numeric_cols = names(numeric_df)

par(mfrow = c(1, length(numeric_cols)))
for (i in numeric_cols){
  boxplot(
    df[i],
    xlab=c(i),
    coef = 1
    )}

```

```{r}
numeric_cols = names(numeric_df)

par(mfrow = c(1, length(numeric_cols)))
for (i in numeric_cols){
  hist(
    df[[i]],
    xlab=c(i)
    )
}
```



```{r}

data.frame(type=sapply(df,class)) %>% filter(type== 'character')
#data.frame(table(df$education_simple))
```

Convert All Categorical Columns to Factors

```{r}

#df$capital_gain = log( df$capital_gain )

df$workclass = factor(df$workclass)

df$education = factor(df$education, levels=
c(' Preschool' ,' 1st-4th' ,' 5th-6th' ,' 7th-8th' ,' 9th' ,' 10th' ,' 11th' ,' 12th' ,' HS-grad' ,' Some-college' ,' Assoc-voc' ,' Assoc-acdm' ,' Bachelors' ,' Masters' ,' Prof-school' ,' Doctorate') )

df$education_simple = factor(df$education_simple, levels=
c( 'Below High School','Some High School','High School','Some College','College','Masters or Above') )

df$marital_status = factor(df$marital_status)
df$occupation = factor(df$occupation)
df$relationship = factor(df$relationship)
df$race = factor(df$race)
df$sex = factor(df$sex)
df$native_country = factor(df$native_country)

df [df$salary==' >50K' , 'y'] = 1
df [df$salary==' <=50K' , 'y'] = 0
df$y = factor(df$y)

df$salary = factor(df$salary, levels = c(' <=50K',' >50K'))

df$married = factor(df$married)

#head(df$salary)
```

```{r}
ggplot(df) +
 aes_string(x = "workclass") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()


ggplot(df) +
 aes_string(x = "education") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "education_simple") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "marital_status") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "relationship") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "race") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "sex") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "salary") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()

ggplot(df) +
 aes_string(x = "married") +
 geom_bar(fill = "#112446") +
 coord_flip() +
 theme_minimal()



```

```{r}

# if you wish to do the model with another software, make sure to use this datat that has been formatted

#write.csv(df,"data/adult_sampled_formatted_pre_model.csv", row.names = FALSE)


```

# Models

Split train and test data

```{r, warning=FALSE}


training.rows <- sample(1:nrow(df),size=0.7*nrow(df))
train_data <- df[training.rows,]
test_data <- df[-training.rows,]


nrow(df)
print(nrow(train_data) )
print(nrow(test_data) )

```

```{r}
#this is where we will store the metrics for each model
model_stats = data.frame(metric = c("Accuracy","True Positive Rate","False Positive Rate","Specificity","Precision","Prevalence"))
```


```{r}
#our variables

#data.frame(type=sapply(df,class))
c(names(df))
```

Naïve Bayes Model

Which variables are important?

Let's start with:

age race native_country

married relationship

workclass occupation hours_per_week

education education_simple

capital_gain capital_loss

age+race+native_country+married+relationship+workclass+occupation+hours_per_week+education_simple+capital_gain+capital_loss

```{r, warning=FALSE}

NBmodel <- naive_bayes(salary ~ age+race+married+relationship+workclass+occupation+hours_per_week+education_simple+capital_gain+capital_loss,
                     data = train_data) 
summary(NBmodel)

```

Score the validation data (predict) using the model. 
Produce a confusion table and an ROC curve for the scored validation data.

```{r, warning=FALSE}
#classification matrix
predNB = predict(NBmodel, test_data, type="prob")[,2] #This is the probability that the score is a "good score"

pred = predNB
pred[pred>=.5] = 1
pred[pred!=1] = 0

classMatrix = table(pred,test_data$salary) #first variable is by row, the second is by column
print("Classification Matrix:")
print(classMatrix)

```


ROC Curve

```{r echo=TRUE}

roc_curve = roc(test_data$salary,predNB,plot = TRUE, print.auc = TRUE)
AUC<- auc(roc_curve)
```

Calculate: accuracy, misclassification rate, true positive rate, false positive rate, specificity, precision, and prevalence.

```{r, warning=FALSE}


#Accuracy score?
print("Accuracy:")
accuracy = sum(diag(classMatrix))/sum(classMatrix)
print(accuracy)

cat("\n")

print("Misclassification Rate")
print(1-accuracy)

cat("\n")

true_negative<-classMatrix[1,1]
false_positive<-classMatrix[2,1]
true_positive<-classMatrix[2,2]
false_negative<-classMatrix[1,2]


print("True Positive Rate")
true_positive_rate = (true_positive)/ (true_positive + false_negative)
print(true_positive_rate)

cat("\n")

print("False Positive Rate")
false_positive_rate = (false_positive)/ (false_positive + true_negative)
print(false_positive_rate)

cat("\n")

print("Specificity:")
specificity<-true_negative/(true_negative+false_negative)
print(specificity)

cat("\n")

print("Precision:")
precision<-true_positive/(true_positive+false_positive)
print(precision)

cat("\n")

print("Prevalence:")
prevalence = (true_positive + false_negative)/(true_negative+false_positive+true_positive+false_negative)
print(prevalence)

#add to model_stats
model_stats$naiveBayes = c(accuracy, true_positive_rate, false_positive_rate, specificity, precision, 1)
model_stats$naiveBayes = round(model_stats$naiveBayes,2)


```


Logit Model

```{r}

LRmodel = glm(salary~
            age+race+married+relationship+workclass+occupation+hours_per_week+education_simple+capital_gain+capital_loss,
            train_data, family = "binomial")

summary(LRmodel)


```

Which variables can we reject the null hypothesis that their coefficients equal zero?

```{r}
varImp(LRmodel) %>% arrange(-Overall)
```


Score the validation data (predict) using the logit model.

```{r}
#classification matrix
predLR = predict(LRmodel,test_data,type="response") #This is the probability that the score is a "good score"
pred = predLR
pred[pred>=.5] = 1
pred[pred!=1] = 0

classMatrix = table(pred,test_data$salary) #first variable is by row, the second is by column
print("Classification Matrix:")
print(classMatrix)


```

ROC Curve

```{r}
roc_curve = roc(test_data$salary,predLR,plot = TRUE, print.auc = TRUE)
AUC<- auc(roc_curve)
```

Calculate: accuracy, misclassification rate, true positive rate, false positive rate, specificity, precision, and prevalence.

```{r}


#Accuracy score?
print("Accuracy:")
accuracy = sum(diag(classMatrix))/sum(classMatrix)
print(accuracy)

cat("\n")

print("Misclassification Rate")
print(1-accuracy)

cat("\n")

true_negative<-classMatrix[1,1]
false_positive<-classMatrix[2,1]
true_positive<-classMatrix[2,2]
false_negative<-classMatrix[1,2]


print("True Positive Rate")
true_positive_rate = (true_positive)/ (true_positive + false_negative)
print(true_positive_rate)

cat("\n")

print("False Positive Rate")
false_positive_rate = (false_positive)/ (false_positive + true_negative)
print(false_positive_rate)

cat("\n")

print("Specificity:")
specificity<-true_negative/(true_negative+false_negative)
print(specificity)

cat("\n")

print("Precision:")
precision<-true_positive/(true_positive+false_positive)
print(precision)

cat("\n")

print("Prevalence:")
prevalence = (true_positive + false_negative)/(true_negative+false_positive+true_positive+false_negative)
print(prevalence)

#add to model_stats
model_stats$Logistic = c(accuracy, true_positive_rate, false_positive_rate, specificity, precision, 1)
model_stats$Logistic = round(model_stats$Logistic,2)

```

Tree Model (CART)

```{r}

adultTree = rpart(y ~ age+race+married+relationship+workclass+occupation+hours_per_week+education_simple+capital_gain+capital_loss, data=train_data, method="class", minbucket = 25)

#summary(adultTree)
```


Variable/feature importance

```{r}
varImp(adultTree) %>% arrange(-Overall)
write.csv(varImp(adultTree) %>% arrange(-Overall), 'data/variable_importance.csv') 
```


Plot of the decision tree.

```{r}

rpart.plot(adultTree, type=4, extra=101, fallen.leaves = TRUE)

```


Score the validation data (predict) using the CART model, produce a confusion table and an ROC curve.

```{r}
PredictCART = predict(adultTree, newdata= test_data,type='prob')[,2]

pred = PredictCART
pred[pred>=.5] = 1
pred[pred!=1] = 0

classMatrix = table(pred,test_data$salary) #first variable is by row, the second is by column
print("Classification Matrix:")
print(classMatrix)
```


```{r, warning=FALSE}
roc_curve = roc(test_data$salary,PredictCART,plot = TRUE, print.auc = TRUE)
AUC<- auc(roc_curve)
```


Calculate: accuracy, misclassification rate, true positive rate, false positive rate, specificity, precision, and prevalence.

```{r}


#Accuracy score?
print("Accuracy:")
accuracy = sum(diag(classMatrix))/sum(classMatrix)
print(accuracy)

cat("\n")

print("Misclassification Rate")
print(1-accuracy)

cat("\n")

true_negative<-classMatrix[1,1]
false_positive<-classMatrix[2,1]
true_positive<-classMatrix[2,2]
false_negative<-classMatrix[1,2]


print("True Positive Rate")
true_positive_rate = (true_positive)/ (true_positive + false_negative)
print(true_positive_rate)

cat("\n")

print("False Positive Rate")
false_positive_rate = (false_positive)/ (false_positive + true_negative)
print(false_positive_rate)

cat("\n")

print("Specificity:")
specificity<-true_negative/(true_negative+false_negative)
print(specificity)

cat("\n")

print("Precision:")
precision<-true_positive/(true_positive+false_positive)
print(precision)

cat("\n")

print("Prevalence:")
prevalence = (true_positive + false_negative)/(true_negative+false_positive+true_positive+false_negative)
print(prevalence)

#add to model_stats
model_stats$CART = c(accuracy, true_positive_rate, false_positive_rate, specificity, precision, 1)
model_stats$CART = round(model_stats$CART,2)

```



```{r}
#Random Forest
modelRF <- randomForest(salary~age+race+married+relationship+workclass+occupation+hours_per_week+education_simple+capital_gain+capital_loss, data = train_data)

```

```{r}

varImp(modelRF) %>% arrange(-Overall)

```


```{r}
PredRF = predict(modelRF, newdata= test_data,type='prob')[,2]
pred = PredRF

pred[pred>=.5] = 1
pred[pred!=1] = 0

classMatrix = table(pred,test_data$salary) #first variable is by row, the second is by column
print("Classification Matrix:")
print(classMatrix)

```



```{r}

roc_curve = roc(test_data$salary,PredRF,plot = TRUE, print.auc = TRUE)
AUC<- auc(roc_curve)

```

Calculate: accuracy, misclassification rate, true positive rate, false positive rate, specificity, precision, and prevalence.

```{r}


#Accuracy score?
print("Accuracy:")
accuracy = sum(diag(classMatrix))/sum(classMatrix)
print(accuracy)

cat("\n")

print("Misclassification Rate")
print(1-accuracy)

cat("\n")

true_negative<-classMatrix[1,1]
false_positive<-classMatrix[2,1]
true_positive<-classMatrix[2,2]
false_negative<-classMatrix[1,2]


print("True Positive Rate")
true_positive_rate = (true_positive)/ (true_positive + false_negative)
print(true_positive_rate)

cat("\n")

print("False Positive Rate")
false_positive_rate = (false_positive)/ (false_positive + true_negative)
print(false_positive_rate)

cat("\n")

print("Specificity:")
specificity<-true_negative/(true_negative+false_negative)
print(specificity)

cat("\n")

print("Precision:")
precision<-true_positive/(true_positive+false_positive)
print(precision)

cat("\n")

print("Prevalence:")
prevalence = (true_positive + false_negative)/(true_negative+false_positive+true_positive+false_negative)
print(prevalence)

#add to model_stats
model_stats$randomForest = c(accuracy, true_positive_rate, false_positive_rate, specificity, precision, 1)
model_stats$randomForest = round(model_stats$randomForest,2)

```


Compare these metrics between all three models.

```{r}

write.csv(model_stats, file='data/model_stats.csv')
model_stats

```










